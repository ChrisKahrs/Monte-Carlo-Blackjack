{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the rank and suit of a card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "ranks = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 10,\n",
    "    \"queen\" : 10,\n",
    "    \"king\" : 10,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "    \n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a card and a deck.\n",
    "\n",
    "Implement shuffle, peek, & deal functions for the deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, suit, rank, value):\n",
    "        self.suit = suit\n",
    "        self.rank = rank\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.rank + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for rank, value in ranks.items():\n",
    "                    self.cards.append(Card(suit, rank, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for evaluating the value of the dealer's hand.\n",
    "\n",
    "Trickiest part is defining the logic for Aces.\n",
    "\n",
    "_Dealer Logic will not change much! They must follow a set, predictable course of action._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This follows the same, official rules every time.\n",
    "# Still need to figure out what happens if there are multiple Aces.\n",
    "def dealer_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.rank == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # use 1 for Ace\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        # See if using 11 instead of 1 for the Aces gets the \n",
    "        # dealer's hand value closer to the [17, 21] range\n",
    "        \n",
    "        # The dealer will follow Hard 17 rules.\n",
    "        # This means the dealer will not hit again if\n",
    "        # the Ace yields a 17. \n",
    "        \n",
    "        # This also means that Aces initially declared as 11's can\n",
    "        # be changed to 1's as new cards come.\n",
    "        \n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            # Only add by 10 b/c 1 is already added before\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 17 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                # The case where even using Ace as eleven is less than 17.\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for evaluating the value of the player's hand.\n",
    "\n",
    "Trickiest part is defining the logic for Aces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    # use_one means that every ace that in the hand is counted as one.\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.rank == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # use 1 for Ace\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        # Define player policy for Aces:\n",
    "        # Make Aces 11 if they get you to the range [18,21]\n",
    "        # Otherwise, use one.\n",
    "        \n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            # Only add by 10 b/c 1 is already added before\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 18 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                # This allows for some Aces to be 11s, and others to be 1.\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for the dealer's turn.\n",
    "\n",
    "This will not change much since the dealer has to follow a defined protocol when making their moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dealer_turn(dealer_hand, deck):\n",
    "    # Calculate dealer hand's value.\n",
    "    dealer_value = dealer_eval(dealer_hand)\n",
    "\n",
    "    # Define dealer policy (is fixed to official rules)\n",
    "\n",
    "    # The dealer keeps hitting until their total is 17 or more\n",
    "    while dealer_value < 17:\n",
    "        # hit\n",
    "        dealer_hand.append(deck.deal())\n",
    "        dealer_value = dealer_eval(dealer_hand)\n",
    "\n",
    "    return dealer_value, dealer_hand, deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the OpenAI Gym Environment for Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        '''\n",
    "        First element of tuple is the range of possible hand values for the player. (3 through 20)\n",
    "        This is the possible range of values that the player will actually have to make a decision for.\n",
    "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
    "        AI Agent. \n",
    "        \n",
    "        However, we also need to add all the hand values that the agent could possibly end up in when\n",
    "        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n",
    "        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n",
    "        makes us have to include the bust-value in the step() function because we need to return that\n",
    "        done is true alongside the final obs, which is the bust-value.\n",
    "        '''\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        # rewards are 0 when the player hits and is still below 21, and they\n",
    "        # keep playing.\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32.4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "\n",
    "    while env.done == False:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, desc = env.step(action)\n",
    "        total_rewards += reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement First-Visit Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_mc() to run the First-Visit Monte Carlo Algorithm\n",
    "\n",
    "This function initializes the key variables for the algorithm and runs the learning algorithm for the AI agent for `num_episodes` episodes. An episode is a simulation of one game of Blackjack using the OpenAI Gym environment defined above.  \n",
    "\n",
    "`Q`, or the Q-table, is a 2-D list in which the rows are the different possible states and the columns are the different possible actions for each state. The values stores in each cell refer to the value, or Q-value, of selecting some action (the column index) given some state (the row index). These Q-values start at 0 for every state-action pair, and are updated by the update_Q() function after each episode to reflect the rewards received in an episode where the state-action pair occured. All Q-values initially start at 0.  \n",
    "\n",
    "`prob`, or the prob table, has the same structure as `Q`, but the cell values refer to the probability of selecting some action (the column index) given some state (the row index). These probabilities are updated by the update_prob() function after the Q-values are updated after each episode. All action probabilties start at 0.5 (or 50%).  \n",
    "\n",
    "`alpha` defines the weight given to each new change in Q-value within the update_Q() function. A smaller `alpha` means that a new reward logged in an episode for some state-action pair has less impact on the current Q-value for that state-action pair. The converse is also true. Therefore, `alpha` essentially defines how fast the AI agent learns.  \n",
    "\n",
    "`epsilon` defines the weight given to each new change in the action probabilities within the update_prob() function. A larger `epsilon` reduces the % amount by which an action probability is changed after some changes in Q-values. The converse is also true. An `epsilon` of 1 means that no change will occur in the action probability, regardless of the magnitude of changes in Q-values.  \n",
    "\n",
    "`epsilon` is decayed by the `decay` value after every episode. The lowest value `epsilon` can reach is `epsilon_min`.  \n",
    "\n",
    "`gamma` is the rate used to discount future rewards yielded by a certain state-action pair in the episode. Since a round (or episode) of Blackjack can have more than 1 decision made, there can be numerous state-action pairs that are seen in one episode. However, only the final decision (or state-action pair) yields an immediate reward from the environment. All previous state-action pairs had no rewards. So, the final reward must be used to modify the Q-values of the earlier state-action pairs. Since the final reward was only partially made possible by the earlier state-action pairs, the final reward is discounted using `gamma` to account for this.  \n",
    "\n",
    "Once these important variables are defined, this function runs the First-Visit Monte Carlo algorithm for the Blackjack environment.\n",
    "\n",
    "The function runs `num_episodes` episodes.  \n",
    "\n",
    "In each episode, `epsilon` is first decayed by the `decay` rate.  \n",
    "\n",
    "Then, the game of Blackjack is played out through the `play_game()` function. This function returns a list of the state-action-reward tuples that occured during the game. These tuples represent the actions the AI agent had to take given some state, and the rewards that resulted.  \n",
    "\n",
    "These tuples are used to modify the Q-values in `Q` through the `update_Q()` function.  \n",
    "\n",
    "Then, the tuples are used to modify the probability distributions in `prob` of the two actions (hit or stand) for any states that were encountered in the episode. This is done through the `update_prob()` function.\n",
    "\n",
    "After this process is done for each episode, the function returns the modified `Q` and `prob` tables. These tables are an imprint of the learning that has taken place by the AI agent through the First-Visit Monte Carlo algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc(env, num_episodes):\n",
    "    '''\n",
    "    observation_space[0] is the 18 possible player values. (3 through 20)\n",
    "    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
    "\n",
    "    Combining these together yields all possible states.\n",
    "\n",
    "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "    This is the Q map.\n",
    "    '''\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1\n",
    "    \n",
    "    # The rate by which epsilon will decay over time.\n",
    "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "    decay = 0.9999\n",
    "    \n",
    "    # The lowest value that epsilon can go to.\n",
    "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "    # running thousands of episodes.\n",
    "    epsilon_min = 0.9\n",
    "\n",
    "    # may have to be tweaked later.\n",
    "    gamma = 0.8\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        episode = play_game(env, Q, epsilon, prob)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    return Q, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play_game() function\n",
    "\n",
    "Simulates playing one round (or episode) or Blackjack.\n",
    "\n",
    "First, use env.reset() to set up a new round of Blackjack.\n",
    "\n",
    "If the player is already dealt a Blackjack by luck, then this episode is just scrapped. There is nothing the AI agent can learn from this episode since it did not have to make any decisions.\n",
    "\n",
    "Otherwise, the AI agent finds the best action available given the current state of the game and its knowledge so far. This knowledge is stored in the `Q`-table defined in the run_mc() function above. Then, the AI agent retrieves the probability that it should take this action from the `prob` table. The AI agent applies this probability and chooses its action given this state. The action is sent to the environment, and a reward is returned from the environment. The state-action-reward sequence that just occured is stored in `episode` as a tuple. This process is repeated until the current episode is over. Each episode will roughly yield 1-3 tuples in `episode` since Blackjack rounds are usually resolved after 1-3 decisions by the player.\n",
    "\n",
    "The Q-values in the `Q`-table associated with each state-action pair that was seen in this episode will be updated after this episode based on the state-action-reward tuples returned by this function. Then, the corresponding probabilities in `prob` are also modified to reflect this change in Q-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q, epsilon, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while env.done == False:\n",
    "        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "        else:\n",
    "            # Get the index in Q that corresponds to the current state\n",
    "            Q_state_index = get_Q_state_index(state)\n",
    "            \n",
    "            # Use the index to get the possible actions, and use np.argmax()\n",
    "            # to get the index of the action that has the highest current Q\n",
    "            # value. Index 0 is hit, index 1 is stand.\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            \n",
    "            # Go to the prob table to retrieve the probability of this action.\n",
    "            # This uses the same Q_state_index used for finding the state index\n",
    "            # of the Q-array.\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "            \n",
    "            # The agent does the action, and we get the next state, the rewards,\n",
    "            # and whether the game is now done.\n",
    "            next_state, reward, env.done, info = env.step(action_to_take)\n",
    "            \n",
    "            # We now have a state->action->reward sequence we can log\n",
    "            # in `episode`\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            \n",
    "            # update the state for the next decision made by the agent.\n",
    "            state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_Q() function\n",
    "\n",
    "This function iterates through the state-action-reward tuples in `episode` and updates the Q-values of the corresponding state-action pairs in `Q`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    '''\n",
    "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "    \n",
    "    If you want first-visit, you need to use the cumulative reward of the entire\n",
    "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "    episode, even the first state/action pair. In this algorithm, an episode\n",
    "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "    the future situations to even occur, so it is important to include the\n",
    "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "    future decisions by lowering gamma, which will lower the G value for our\n",
    "    early state/action pair in which we hit and did not get any immediate rewards.\n",
    "    This will make our agent consider future rewards, and not just look at \n",
    "    each state in isolation despite having hit previously.\n",
    "     \n",
    "    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
    "    and just use the immediate reward in this episode for each state/action pair.\n",
    "    '''\n",
    "    step = 0\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for curr_step in range(step, len(episode)):\n",
    "            total_reward += (gamma ** gamma_exp) * episode[curr_step][2]\n",
    "            gamma_exp += 1\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(state)\n",
    "        Q[Q_state_index][action] = Q[Q_state_index][action] + alpha * (total_reward - Q[Q_state_index][action])\n",
    "        \n",
    "        # update step to start further down the episode next time.\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update_prob() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state. The goal is that the new update in Q has changed what the best action\n",
    "        # is, and epsilon will be used to create a small increase in the probability\n",
    "        # that the new, better action is chosen.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to map a state from the Blackjack environment to the proper index in `Q` and `prob` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a state, derive the corresponding index in the Q-array.\n",
    "# The state is a player hand value + dealer upcard pair,\n",
    "# so a \"hashing\" formula must be used to allocate the\n",
    "# indices of the Q-array properly.\n",
    "def get_Q_state_index(state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to get and update the probability of taking the best action for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array,\n",
    "    # which stores the probability of taking an action (hit or stand)\n",
    "    # for a given state/action pair.\n",
    "    return prob[Q_state_index][best_action]\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Slightly alter the probability of this best action being taken by using epsilon\n",
    "    # Epsilon starts at 1.0, and slowly decays over time.\n",
    "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "    # As time goes on, the likelihood that the best action is taken is increased from\n",
    "    # what is listed in the `prob` array.\n",
    "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "    # but exploitation later for a greater reward.\n",
    "    #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
    "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run First-Visit Monte Carlo Reinforcement Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()\n",
    "new_Q, new_prob = run_mc(env, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best_policy() function\n",
    "This function takes the new_Q Q-table returned by the First Visit MC algorithm and keeps only the action that yields the highest value for a given state.  \n",
    "\n",
    "This is represented in three different ways: binary, string, and colors.  \n",
    "0 is hit and 1 is stand in the binary representation of the results.  \n",
    "H is hit and S is stand in the binary representation of the results.  \n",
    "Green is hit and Red is stand in the binary representation of the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(Q):\n",
    "    best_policy_binary = []\n",
    "    best_policy_string = []\n",
    "    best_policy_colors = []\n",
    "    for i in range(len(Q)):\n",
    "        best_policy_binary.append(np.argmax(Q[i]))\n",
    "        best_policy_string.append(\"H\" if np.argmax(Q[i]) == 0 else \"S\")\n",
    "        best_policy_colors.append(\"g\" if np.argmax(Q[i]) == 0 else \"r\")\n",
    "        \n",
    "    return best_policy_binary, best_policy_string, best_policy_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame w/ Player Value as Rows & Dealer Upcard as Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_Q_binary, new_Q_string, new_Q_colors = best_policy(new_Q)\n",
    "\n",
    "df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "color_df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "for s in range(3, 21): # possible player values in the range 3 to 20\n",
    "    start = env.observation_space[1].n * (s-3)\n",
    "    end = start + 10\n",
    "    df.loc[s]=(new_Q_string[start:end])\n",
    "    color_df.loc[s]=(new_Q_colors[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chart Graphic for the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.set_axis_off()\n",
    "ax.axis('tight')\n",
    "\n",
    "ax.table(cellText=df.values, cellColours=color_df.values, cellLoc=\"center\", rowLabels=df.index, colLabels=df.columns, loc='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Best Policy on New Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25.8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    while env.done == False:\n",
    "        if state[0] == 19: #Player was dealt Blackjack\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            total_rewards += reward\n",
    "        else:\n",
    "            Q_index = get_Q_state_index(state)\n",
    "            action = new_Q_binary[Q_index]\n",
    "\n",
    "            new_state, reward, done, desc = env.step(action)\n",
    "            total_rewards += reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
