{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the rank and suit of a card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "ranks = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 10,\n",
    "    \"queen\" : 10,\n",
    "    \"king\" : 10,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "    \n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a card and a deck.\n",
    "\n",
    "Implement shuffle, peek, & deal functions for the deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, suit, rank, value):\n",
    "        self.suit = suit\n",
    "        self.rank = rank\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.rank + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for rank, value in ranks.items():\n",
    "                    self.cards.append(Card(suit, rank, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for evaluating the value of the dealer's hand.\n",
    "\n",
    "Trickiest part is defining the logic for Aces.\n",
    "\n",
    "_Dealer Logic will not change much! They must follow a set, predictable course of action._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This follows the same, official rules every time.\n",
    "# Still need to figure out what happens if there are multiple Aces.\n",
    "def dealer_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.rank == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # use 1 for Ace\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        # See if using 11 instead of 1 for the Aces gets the \n",
    "        # dealer's hand value closer to the [17, 21] range\n",
    "        \n",
    "        # The dealer will follow Hard 17 rules.\n",
    "        # This means the dealer will not hit again if\n",
    "        # the Ace yields a 17. \n",
    "        \n",
    "        # This also means that Aces initially declared as 11's can\n",
    "        # be changed to 1's as new cards come.\n",
    "        \n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            # Only add by 10 b/c 1 is already added before\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 17 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                # The case where even using Ace as eleven is less than 17.\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for evaluating the value of the player's hand.\n",
    "\n",
    "Trickiest part is defining the logic for Aces.\n",
    "\n",
    "_This logic is subject to change._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_eval(player_hand):\n",
    "    num_ace = 0\n",
    "    # use_one means that every ace that in the hand is counted as one.\n",
    "    use_one = 0\n",
    "    for card in player_hand:\n",
    "        if card.rank == \"ace\":\n",
    "            num_ace += 1\n",
    "            use_one += card.value[0] # use 1 for Ace\n",
    "        else:\n",
    "            use_one += card.value\n",
    "    \n",
    "    if num_ace > 0:\n",
    "        # Define player policy for Aces:\n",
    "        # Make Aces 11 if they get you to the range [18,21]\n",
    "        # Otherwise, use one.\n",
    "        \n",
    "        ace_counter = 0\n",
    "        while ace_counter < num_ace:\n",
    "            # Only add by 10 b/c 1 is already added before\n",
    "            use_eleven = use_one + 10 \n",
    "            \n",
    "            if use_eleven > 21:\n",
    "                return use_one\n",
    "            elif use_eleven >= 18 and use_eleven <= 21:\n",
    "                return use_eleven\n",
    "            else:\n",
    "                # This allows for some Aces to be 11s, and others to be 1.\n",
    "                use_one = use_eleven\n",
    "            \n",
    "            ace_counter += 1\n",
    "        \n",
    "        return use_one\n",
    "    else:\n",
    "        return use_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define logic for the dealer's turn.\n",
    "\n",
    "This will not change much since the dealer has to follow a defined protocol when making their moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dealer_turn(dealer_hand, deck):\n",
    "    # Calculate dealer hand's value.\n",
    "    dealer_value = dealer_eval(dealer_hand)\n",
    "\n",
    "    # Define dealer policy (is fixed to official rules)\n",
    "\n",
    "    # The dealer keeps hitting until their total is 17 or more\n",
    "    while dealer_value < 17:\n",
    "        # hit\n",
    "        dealer_hand.append(deck.deal())\n",
    "        dealer_value = dealer_eval(dealer_hand)\n",
    "\n",
    "    return dealer_value, dealer_hand, deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the OpenAI Gym Environment for Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "ROUNDS_PER_EPISODE = 100\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        '''\n",
    "        First element of tuple is the range of possible hand values for the player. (2 through 20)\n",
    "        This is the possible range of values that the player will actually have to make a decision for.\n",
    "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
    "        AI Agent. \n",
    "        \n",
    "        However, we also need to add all the hand values that the agent could possibly end up in when\n",
    "        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n",
    "        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n",
    "        makes us have to include the bust-value in the step() function because we need to return that\n",
    "        done is true alongside the final obs, which is the bust-value.\n",
    "        \n",
    "        Therefore, we will also be adding the values 21 through 30 in our observation space. 30 is the\n",
    "        highest possible hand value we can bust at. It happens when the agent hits at a hand value of \n",
    "        20 and gets a 10/J/Q/K. Getting an Ace in this position results in Blackjack for the agent\n",
    "        because the Ace will just be used as a 1, not an 11.\n",
    "        \n",
    "        Therefore, the total observation space is a discrete range from 2 through 30. Therefore, the range\n",
    "        is really 29 discrete numbers, and we will have to adjust the hand values to fit into the range\n",
    "        1-29 by subtracting our hand value by 1.\n",
    "        '''\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (1 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(19), spaces.Discrete(11)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # define the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        rewards = 0\n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        player_value_obs = self.player_value - 1\n",
    "        \n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Our Blackjack deck will be made of 2 normal decks by default.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        player_value_obs = self.player_value - 1\n",
    "            \n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        \n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "\n",
    "#     env.render()\n",
    "#     print()\n",
    "    while env.done == False:\n",
    "        action = env.action_space.sample()\n",
    "#         print(\"Action: \", action)\n",
    "\n",
    "#         print()\n",
    "\n",
    "        new_state, reward, done, desc = env.step(action)\n",
    "        total_rewards += reward\n",
    "#         env.render()\n",
    "#         print()\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-Visit Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 1.0010e-01 -1.9995e-01]\n",
      " [ 2.9907e-01  0.0000e+00]\n",
      " [-9.9976e-02 -1.0010e-01]\n",
      " [ 1.9983e-01 -9.9854e-02]\n",
      " [-9.9976e-02 -2.0020e-01]\n",
      " [-9.9976e-02  0.0000e+00]\n",
      " [-1.9897e-01 -1.9983e-01]\n",
      " [-1.2434e-04 -9.9976e-02]\n",
      " [-1.9788e-01 -5.9766e-01]\n",
      " [-9.9976e-02 -9.9976e-02]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 9.8633e-02  0.0000e+00]\n",
      " [-2.9980e-01 -2.9907e-01]\n",
      " [-2.9907e-01  4.9019e-04]\n",
      " [ 9.8999e-02 -9.9976e-02]\n",
      " [ 2.9736e-01 -1.2434e-04]\n",
      " [-2.9907e-01 -9.9976e-02]\n",
      " [-3.9917e-01 -3.9917e-01]\n",
      " [-1.0010e-01 -1.9983e-01]\n",
      " [-1.4854e+00 -1.6807e+00]\n",
      " [-1.9983e-01 -1.9958e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 1.0034e-01 -2.9980e-01]\n",
      " [-2.9956e-01 -4.9731e-01]\n",
      " [-1.9873e-01 -2.9956e-01]\n",
      " [ 2.4438e-06 -1.9983e-01]\n",
      " [-2.9956e-01  7.9688e-01]\n",
      " [-2.9932e-01 -3.9917e-01]\n",
      " [-5.9766e-01 -4.9878e-01]\n",
      " [-2.9956e-01 -9.9609e-02]\n",
      " [-1.3828e+00 -9.7803e-01]\n",
      " [-3.9917e-01 -1.9983e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.0071e-01  1.0022e-01]\n",
      " [-9.9976e-02 -9.9976e-02]\n",
      " [-9.9976e-02 -1.0046e-01]\n",
      " [ 5.9473e-01  0.0000e+00]\n",
      " [-3.9893e-01  2.0190e-01]\n",
      " [ 5.9717e-01 -1.2434e-04]\n",
      " [ 3.9624e-01  0.0000e+00]\n",
      " [-4.9878e-01 -2.9956e-01]\n",
      " [-2.2461e+00 -2.1738e+00]\n",
      " [-1.0928e+00 -9.9414e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-9.9976e-02  9.7717e-02]\n",
      " [ 2.9346e-01  0.0000e+00]\n",
      " [-1.9983e-01 -9.7168e-02]\n",
      " [-1.9983e-01  4.9316e-01]\n",
      " [ 2.8931e-01  0.0000e+00]\n",
      " [-2.0007e-01 -1.9983e-01]\n",
      " [-5.9717e-01 -4.9512e-01]\n",
      " [-4.9292e-01 -4.9878e-01]\n",
      " [-1.7725e+00 -2.3340e+00]\n",
      " [-9.9023e-01 -6.9727e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 1.0022e-01 -3.9941e-01]\n",
      " [ 1.0352e-01 -5.9570e-01]\n",
      " [-2.0020e-01 -1.9983e-01]\n",
      " [-9.9976e-02  5.9180e-01]\n",
      " [ 9.8083e-02 -1.0046e-01]\n",
      " [ 1.8652e-01  0.0000e+00]\n",
      " [ 4.9365e-01 -1.9983e-01]\n",
      " [ 4.7569e-03 -3.9917e-01]\n",
      " [-1.4600e+00 -1.3789e+00]\n",
      " [-7.9395e-01 -7.9688e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 5.8936e-01 -9.9976e-02]\n",
      " [ 3.8940e-01 -1.9983e-01]\n",
      " [-2.9956e-01  3.9844e-01]\n",
      " [ 1.0898e+00 -9.9976e-02]\n",
      " [ 1.3691e+00 -9.9976e-02]\n",
      " [ 8.9502e-01 -2.9956e-01]\n",
      " [ 2.9956e-01 -1.0010e-01]\n",
      " [ 7.8955e-01 -9.9976e-02]\n",
      " [-2.1797e+00 -2.1621e+00]\n",
      " [-1.0859e+00 -8.9502e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [ 6.9824e-01 -3.9917e-01]\n",
      " [-9.9854e-02  6.7285e-01]\n",
      " [ 6.8701e-01 -9.9976e-02]\n",
      " [ 2.4590e+00  9.8389e-02]\n",
      " [ 1.2637e+00  9.9976e-02]\n",
      " [ 5.8350e-01 -2.9956e-01]\n",
      " [ 1.3740e+00 -1.9983e-01]\n",
      " [ 2.9663e-01 -1.9971e-01]\n",
      " [ 4.0723e-01 -1.9983e-01]\n",
      " [ 5.8057e-01 -1.9922e-01]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.8340e+00 -1.3818e+00]\n",
      " [-1.5439e+00 -1.8525e+00]\n",
      " [-4.7852e-01 -9.8877e-01]\n",
      " [-1.0771e+00 -1.0840e+00]\n",
      " [-3.7061e-01 -6.9775e-01]\n",
      " [-7.8613e-01 -1.0879e+00]\n",
      " [-1.3447e+00 -9.9219e-01]\n",
      " [-1.5586e+00 -1.9766e+00]\n",
      " [-8.4531e+00 -7.5586e+00]\n",
      " [-3.6113e+00 -3.6895e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.0645e+00 -1.7578e+00]\n",
      " [-1.5615e+00 -1.6572e+00]\n",
      " [-1.5752e+00 -1.4316e+00]\n",
      " [-9.7900e-01 -4.7607e-01]\n",
      " [-6.9629e-01 -5.7471e-01]\n",
      " [-1.4658e+00 -1.0762e+00]\n",
      " [-1.7656e+00 -1.7617e+00]\n",
      " [-2.3359e+00 -2.0625e+00]\n",
      " [-8.6797e+00 -8.4766e+00]\n",
      " [-3.3164e+00 -3.2422e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.1846e+00 -2.1504e+00]\n",
      " [-2.9761e-01 -7.8564e-01]\n",
      " [-8.9258e-01 -1.2686e+00]\n",
      " [-5.9375e-01 -3.8477e-01]\n",
      " [-2.9907e-01 -3.9600e-01]\n",
      " [-4.7461e-01 -1.6758e+00]\n",
      " [-2.4336e+00 -2.0605e+00]\n",
      " [-1.4775e+00 -1.2793e+00]\n",
      " [-8.5859e+00 -7.6250e+00]\n",
      " [-2.9336e+00 -3.4336e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.4834e+00 -9.6826e-01]\n",
      " [-8.9307e-01 -1.4414e+00]\n",
      " [-1.2607e+00 -8.9209e-01]\n",
      " [-1.9983e-01 -1.0400e-01]\n",
      " [-5.9619e-01 -4.6777e-01]\n",
      " [-1.6758e+00 -2.1387e+00]\n",
      " [-1.8672e+00 -2.1289e+00]\n",
      " [-1.6914e+00 -1.9561e+00]\n",
      " [-8.4688e+00 -8.3750e+00]\n",
      " [-2.9414e+00 -2.9414e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.4824e+00 -1.1797e+00]\n",
      " [-2.4531e+00 -2.3340e+00]\n",
      " [-1.4600e+00 -8.7744e-01]\n",
      " [-6.9287e-01 -1.0752e+00]\n",
      " [-9.9609e-02 -1.1993e-01]\n",
      " [-1.9482e+00 -1.7471e+00]\n",
      " [-1.7646e+00 -1.4766e+00]\n",
      " [-1.3867e+00 -1.9541e+00]\n",
      " [-8.9141e+00 -9.3281e+00]\n",
      " [-4.3867e+00 -4.4023e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.9424e+00 -1.3789e+00]\n",
      " [-1.1855e+00 -6.9092e-01]\n",
      " [-4.9854e-01  7.9193e-03]\n",
      " [-9.9854e-02  2.8687e-01]\n",
      " [-3.9941e-01  1.5625e+00]\n",
      " [-1.1885e+00 -9.6533e-01]\n",
      " [-1.7559e+00 -1.9502e+00]\n",
      " [-2.1602e+00 -2.1387e+00]\n",
      " [-8.8594e+00 -8.9062e+00]\n",
      " [-3.0352e+00 -2.9453e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-1.9995e-01 -1.3168e-02]\n",
      " [-9.9609e-02  1.3643e+00]\n",
      " [-1.9983e-01  5.3857e-01]\n",
      " [-2.9956e-01  1.2744e+00]\n",
      " [-2.9907e-01  3.0859e+00]\n",
      " [-9.9976e-02  2.7930e+00]\n",
      " [-7.9688e-01 -5.6787e-01]\n",
      " [-8.9160e-01 -9.7266e-01]\n",
      " [-7.1367e+00 -7.7148e+00]\n",
      " [-2.2676e+00 -2.3574e+00]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-2.9980e-01  1.4590e+00]\n",
      " [-1.0022e-01  3.0254e+00]\n",
      " [-1.9995e-01  2.1230e+00]\n",
      " [-5.9814e-01  1.4834e+00]\n",
      " [-9.9854e-02  2.6816e+00]\n",
      " [-3.9917e-01  7.2695e+00]\n",
      " [-1.9983e-01  4.9141e+00]\n",
      " [-2.9956e-01  3.8086e+00]\n",
      " [-1.9897e-01 -4.0454e-01]\n",
      " [-2.9956e-01 -2.2202e-02]\n",
      " [ 0.0000e+00  0.0000e+00]\n",
      " [-3.9917e-01  7.4492e+00]\n",
      " [-3.9917e-01  4.4336e+00]\n",
      " [-9.9976e-02  5.1133e+00]\n",
      " [-4.9854e-01  6.2500e+00]\n",
      " [-3.9917e-01  7.8750e+00]\n",
      " [-4.9878e-01  9.1797e+00]\n",
      " [-1.9983e-01  9.0938e+00]\n",
      " [-1.2434e-04  9.4609e+00]\n",
      " [-5.9814e-01  1.4781e+01]\n",
      " [-2.9932e-01  2.6914e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_Q_state_index(env, state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)\n",
    "\n",
    "def get_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(env, state)\n",
    "    \n",
    "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array,\n",
    "    # which stores the probability of taking an action (hit or stand)\n",
    "    # for a given state/action pair.\n",
    "    return prob[Q_state_index][best_action] + 1 - epsilon\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(env, state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Slightly alter the probability of this best action being taken by using epsilon\n",
    "    # Epsilon starts at 1.0, and slowly decays over time.\n",
    "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "    # As time goes on, the likelihood that the best action is taken is increased from\n",
    "    # what is listed in the `prob` array.\n",
    "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "    # but exploitation later for a greater reward.\n",
    "    prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + 1 - epsilon\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\\\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def play_game(env, Q, epsilon, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while env.done == False:\n",
    "        if state[0] == 20: #Player was dealt Blackjack\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "        else:\n",
    "            Q_state_index = get_Q_state_index(env, state)\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "\n",
    "            next_state, reward, env.done, info = env.step(action_to_take)\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            state = next_state\n",
    "        \n",
    "    return episode\n",
    "    \n",
    "\n",
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    '''\n",
    "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "    \n",
    "    If you want first-visit, you need to use the cumulative reward of the entire\n",
    "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "    episode, even the first state/action pair. In this algorithm, an episode\n",
    "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "    the future situations to even occur, so it is important to include the\n",
    "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "    future decisions by lowering gamma, which will lower the G value for our\n",
    "    early state/action pair in which we hit and did not get any immediate rewards.\n",
    "    This will make our agent consider future rewards, and not just look at \n",
    "    each state in isolation despite having hit previously.\n",
    "     \n",
    "    If you want every visit, do not use the cumulative rewards when updating Q-values,\n",
    "    and just use the immediate reward in this episode for each state/action pair.\n",
    "    '''\n",
    "    step = 0\n",
    "    #env.render()\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for step in range(step, len(episode)):\n",
    "            total_reward += (gamma ** gamma_exp) * episode[step][2]\n",
    "            gamma_exp += 1\n",
    "    \n",
    "#         print(state)\n",
    "#         print(action)\n",
    "#         print(reward)\n",
    "#         print(total_reward)\n",
    "#         print()\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(env, state)\n",
    "        Q[Q_state_index][action] = Q[Q_state_index][action] + alpha * (total_reward - Q[Q_state_index][action])\n",
    "        \n",
    "        \n",
    "    return Q\n",
    "\n",
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state. The goal is that the new update in Q has changed what the best action\n",
    "        # is, and epsilon will be used to create a small increase in the probability\n",
    "        # that the new, better action is chosen.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob\n",
    "\n",
    "def run_mc(env, num_episodes):\n",
    "    '''\n",
    "    observation_space[0] is the 29 possible player values.\n",
    "    observation_space[1] is the 11 possible dealer upcards.\n",
    "\n",
    "    Combining these together yields all possible states.\n",
    "\n",
    "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "    This is the Q map.\n",
    "    '''\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1.0\n",
    "    # The rate by which epsilon will decay over time.\n",
    "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "    decay = 0.9999\n",
    "    # The lowest value that epsilon can go to.\n",
    "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "    # running thousands of episodes.\n",
    "    epsilon_min = 0.1\n",
    "\n",
    "    # may have to be tweaked later.\n",
    "    gamma = 1.0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        episode = play_game(env, Q, epsilon, prob)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    # still need to write best_policy method\n",
    "    return Q\n",
    "        \n",
    "env = BlackjackEnv()\n",
    "new_Q = run_mc(env, 10000)\n",
    "   \n",
    "print(new_Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
